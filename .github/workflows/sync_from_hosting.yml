name: Mirror hosting -> GitHub (recursive /rizwan)

on:
  schedule:
    - cron: "*/1 * * * *"
  workflow_dispatch:

concurrency:
  group: mirror-rizwan
  cancel-in-progress: true

jobs:
  sync:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Recursive mirror (200=upload/keep, 404=delete, SHA=update)
        shell: bash
        run: |
          set -euo pipefail

          HOST_BASE="https://nskmedia.net/rizwan"
          REPO_PREFIX="rizwan"   # GitHub me yahan mirror banega: rizwan/...

          UA="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36"

          # If root listing is blocked, we still crawl these known dirs (from your screenshot)
          SEED_DIRS=(4.2 BGMI BPSTOPHERE GLITCH LDPLAYER LOG TEST1 TEST2 TEST3 TEST4)

          # Safety limits (avoid infinite loops / huge trees)
          MAX_DEPTH=6
          MAX_FILES=5000

          shopt -s extglob nullglob

          declare -A VISITED_DIRS
          declare -A SEEN_FILES
          files_count=0

          # -------- helpers --------
          norm_rel() {
            local p="$1"
            p="${p#./}"
            p="${p#/}"
            echo "$p"
          }

          fetch_listing() {
            # outputs html path if 200, else empty
            local url="$1"
            local html
            html="$(mktemp)"
            local code
            code="$(curl -L -A "$UA" -sS -o "$html" -w "%{http_code}" "${url}/" || true)"
            if [[ "$code" != "200" ]]; then
              rm -f "$html"
              return 1
            fi
            echo "$html"
            return 0
          }

          # Parse links from common Apache/cPanel autoindex HTML:
          # - dirs end with /
          # - files have dot/ext etc
          parse_links() {
            local html="$1"
            # get href targets
            grep -Eo 'href="[^"]+"' "$html" \
              | sed 's/^href="//; s/"$//' \
              | sed 's/%20/ /g'
          }

          sync_file() {
            local url="$1"
            local dest="$2"

            mkdir -p "$(dirname "$dest")"
            local tmp
            tmp="$(mktemp)"

            local code
            code="$(curl -L -A "$UA" -sS -o "$tmp" -w "%{http_code}" "$url" || true)"

            echo "FILE URL : $url"
            echo "DEST     : $dest"
            echo "HTTP     : $code"

            # Hosting 404 -> delete from GitHub
            if [[ "$code" == "404" ]]; then
              rm -f "$tmp"
              if [[ -f "$dest" ]]; then
                rm -f "$dest"
                echo "HOST 404 -> DELETED: $dest"
              else
                echo "HOST 404 -> already missing: $dest"
              fi
              return 0
            fi

            # Non-200/404 -> safe skip
            if [[ "$code" != "200" ]]; then
              rm -f "$tmp"
              echo "HOST HTTP $code -> SKIP"
              return 0
            fi

            # 200 -> upload/update by SHA
            if [[ ! -f "$dest" ]]; then
              mv -f "$tmp" "$dest"
              echo "HOST 200 + missing -> UPLOADED: $dest"
              return 0
            fi

            local sha_new sha_old
            sha_new="$(sha256sum "$tmp" | awk '{print $1}')"
            sha_old="$(sha256sum "$dest" | awk '{print $1}')"

            if [[ "$sha_new" == "$sha_old" ]]; then
              rm -f "$tmp"
              echo "SAME SHA -> NO CHANGE: $dest"
              return 0
            fi

            mv -f "$tmp" "$dest"
            echo "SHA changed -> UPDATED: $dest"
          }

          crawl_dir() {
            local rel="$1"
            local depth="$2"

            rel="$(norm_rel "$rel")"
            [[ -z "$rel" ]] && rel="."

            # depth limit
            if (( depth > MAX_DEPTH )); then
              echo "DEPTH LIMIT reached at: $rel"
              return 0
            fi

            # avoid cycles
            if [[ -n "${VISITED_DIRS[$rel]:-}" ]]; then
              return 0
            fi
            VISITED_DIRS[$rel]=1

            local url
            if [[ "$rel" == "." ]]; then
              url="$HOST_BASE"
            else
              url="$HOST_BASE/$rel"
            fi

            local html
            if ! html="$(fetch_listing "$url")"; then
              echo "No listing (blocked or not 200) -> skip dir discovery: $url"
              return 0
            fi

            echo "---- LISTING OK: $url/"

            # read all hrefs
            while IFS= read -r href; do
              # skip empties
              [[ -z "$href" ]] && continue

              # skip parent/self/query/absolute
              [[ "$href" == "../" ]] && continue
              [[ "$href" == "./" ]] && continue
              [[ "$href" == \?* ]] && continue
              [[ "$href" == /* ]] && continue
              [[ "$href" == http* ]] && continue

              # directory
              if [[ "$href" == */ ]]; then
                local sub="${href%/}"
                local next
                if [[ "$rel" == "." ]]; then
                  next="$sub"
                else
                  next="$rel/$sub"
                fi
                crawl_dir "$next" $((depth+1))
                continue
              fi

              # file
              local file_rel
              if [[ "$rel" == "." ]]; then
                file_rel="$href"
              else
                file_rel="$rel/$href"
              fi

              files_count=$((files_count+1))
              if (( files_count > MAX_FILES )); then
                echo "FILE LIMIT reached (${MAX_FILES}). Stopping."
                rm -f "$html"
                return 0
              fi

              # mark as seen
              SEEN_FILES["$file_rel"]=1

              # sync it
              local file_url="$HOST_BASE/$file_rel"
              local dest_path="$REPO_PREFIX/$file_rel"
              sync_file "$file_url" "$dest_path"

            done < <(parse_links "$html")

            rm -f "$html"
          }

          echo "=== START: recursive mirror of /rizwan ==="

          # 1) try crawl root (best case)
          crawl_dir "." 0

          # 2) also crawl seed dirs (in case root listing blocked)
          for d in "${SEED_DIRS[@]}"; do
            crawl_dir "$d" 1
          done

          echo "=== DISCOVERY DONE. Seen files: ${#SEEN_FILES[@]} ==="

          # Delete files that are in GitHub but not on hosting listing (only if we saw something)
          if (( ${#SEEN_FILES[@]} > 0 )); then
            echo "=== DELETE missing files in repo under ${REPO_PREFIX}/ ==="
            if [[ -d "$REPO_PREFIX" ]]; then
              while IFS= read -r f; do
                # convert repo path to hosting relative path
                rel="${f#${REPO_PREFIX}/}"
                rel="$(norm_rel "$rel")"
                if [[ -z "${SEEN_FILES[$rel]:-}" ]]; then
                  echo "NOT ON HOST LIST -> deleting from repo: $f"
                  rm -f "$f"
                fi
              done < <(find "$REPO_PREFIX" -type f)
            fi
          else
            echo "No file listing discovered (SEEN=0) -> skip mass delete for safety."
          fi

      - name: Commit & push if changed (rebase + retry)
        shell: bash
        run: |
          set -euo pipefail

          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          git add -A

          if git diff --cached --quiet; then
            echo "No changes"
            exit 0
          fi

          git commit -m "Mirror hosting /rizwan (recursive)" || true

          for i in 1 2 3; do
            git fetch origin main
            git rebase origin/main || (git rebase --abort && git pull --rebase origin main)
            if git push origin HEAD:main; then
              echo "Push success"
              exit 0
            fi
            echo "Push failed, retrying... ($i)"
            sleep 2
          done

          echo "Push failed after retries"
          exit 1
